#!/bin/bash
set -e

echo "=== TensorRT-LLM Inference Server Startup ==="
echo "Model: {{ model_id }}"

# Configure HuggingFace to use local models only (no network access)
# Models are stored in MLflow artifacts on JuiceFS at /mlflow-models/artifacts/{run_id}/artifacts/model
export HF_HUB_OFFLINE=1
export HF_HOME="/mlflow-models"

echo "HuggingFace offline mode enabled (HF_HUB_OFFLINE=1)"
echo "Models will be loaded from MLflow artifacts on JuiceFS"

# Query MLflow to get the run_id for the latest version of this model
echo "Querying MLflow for model location..."
MODEL_PATH=$(python3 << 'PYTHON_SCRIPT'
import os
import sys
import requests

# Get auth token
token_url = os.environ['MLFLOW_KEYCLOAK_TOKEN_URL']
token_response = requests.post(
    token_url,
    data={
        'grant_type': 'password',
        'client_id': os.environ['MLFLOW_KEYCLOAK_CLIENT_ID'],
        'client_secret': os.environ['MLFLOW_CLIENT_SECRET'],
        'username': os.environ['MLFLOW_AUTH_USERNAME'],
        'password': os.environ['MLFLOW_AUTH_PASSWORD'],
        'scope': 'openid'
    },
    verify=False,
    timeout=30
)
token_response.raise_for_status()
access_token = token_response.json()['access_token']

# Query MLflow for model
model_id = "{{ model_id }}"
model_name = model_id.replace('/', '-')
mlflow_url = os.environ.get('MLFLOW_TRACKING_URI', 'http://mlflow.mlflow.svc.cluster.local:5000')

response = requests.get(
    f"{mlflow_url}/api/2.0/mlflow/model-versions/search",
    params={'filter': f"name='{model_name}'"},
    headers={'Authorization': f'Bearer {access_token}'},
    verify=False,
    timeout=30
)
response.raise_for_status()

versions = response.json().get('model_versions', [])
if not versions:
    print(f"ERROR: Model {model_name} not found in MLflow registry", file=sys.stderr)
    sys.exit(1)

# Get latest version
latest = max(versions, key=lambda v: int(v['version']))
run_id = latest['run_id']

print(f"Found model version {latest['version']} with run_id: {run_id}", file=sys.stderr)

# Get artifact path from run_id
# Models are stored at: /mlflow-models/artifacts/{run_id}/artifacts/model
model_path = f'/mlflow-models/artifacts/{run_id}/artifacts/model'

# Verify the path exists
if not os.path.exists(model_path):
    print(f"ERROR: Model path does not exist: {model_path}", file=sys.stderr)
    sys.exit(1)

print(f"✓ Model found at: {model_path}", file=sys.stderr)

# Output the path to stdout (captured by shell)
print(model_path)
PYTHON_SCRIPT
)

if [ $? -ne 0 ]; then
    echo "ERROR: Failed to locate model in MLflow"
    exit 1
fi

echo "Using model path: $MODEL_PATH"

# Create TensorRT-LLM API config
cat > /tmp/extra-llm-api-config.yml <<EOF
print_iter_log: false
kv_cache_config:
  dtype: "auto"
  free_gpu_memory_fraction: 0.9
cuda_graph_config:
  enable_padding: true
disable_overlap_scheduler: true
EOF

# Start TensorRT-LLM server
# Model is loaded directly from JuiceFS path - no download needed!
echo "Starting TensorRT-LLM server..."
trtllm-serve "$MODEL_PATH" \
  --backend pytorch \
  --max_batch_size 64 \
  --trust_remote_code \
  --port 8355 \
  --host 0.0.0.0 \
  --extra_llm_api_options /tmp/extra-llm-api-config.yml &

# Wait for server to be ready (30 minutes for large models)
echo "Waiting for TensorRT-LLM server to start..."
for i in {1..900}; do
    if curl -s http://localhost:8355/v1/models >/dev/null 2>&1; then
        echo "✓ TensorRT-LLM server is ready"
        break
    fi
    if [ $i -eq 900 ]; then
        echo "ERROR: TensorRT-LLM server failed to start (timeout after 30 minutes)"
        exit 1
    fi
    sleep 2
done

# Start Gradio UI
echo "Starting Gradio UI on port 7860..."
exec python3 server.py
