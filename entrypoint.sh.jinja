#!/bin/bash
set -e

echo "=== TensorRT-LLM Inference Server Startup (MLflow) ==="
echo "MLflow Model: {{ mlflow_model }}"

# Query MLflow for model S3 location
echo "Querying MLflow Model Registry..."
MODEL_S3_PATH=$(python3 -c "
import os
import sys
import requests
import mlflow
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Get credentials from environment
keycloak_secret = os.environ.get('MLFLOW_CLIENT_SECRET')
admin_password = os.environ.get('ADMIN_PASSWORD')
domain = '{{ domain_name }}'

if not keycloak_secret or not admin_password:
    print('ERROR: Missing MLFLOW_CLIENT_SECRET or ADMIN_PASSWORD', file=sys.stderr)
    sys.exit(1)

# Authenticate with Keycloak
token_url = f'https://auth.{domain}/realms/thinkube/protocol/openid-connect/token'
try:
    response = requests.post(token_url, data={
        'grant_type': 'password',
        'client_id': 'mlflow',
        'client_secret': keycloak_secret,
        'username': 'thinkube',
        'password': admin_password
    }, verify=False, timeout=30)
    response.raise_for_status()
    token = response.json()['access_token']
except Exception as e:
    print(f'ERROR: Failed to authenticate with Keycloak: {e}', file=sys.stderr)
    sys.exit(1)

# Configure MLflow
os.environ['MLFLOW_TRACKING_TOKEN'] = token
os.environ['MLFLOW_TRACKING_URI'] = 'http://mlflow.mlflow.svc.cluster.local:5000'
os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://seaweedfs-filer.seaweedfs.svc.cluster.local:8333'
os.environ['MLFLOW_S3_IGNORE_TLS'] = 'true'
os.environ['AWS_ACCESS_KEY_ID'] = 'seaweedfs'
os.environ['AWS_SECRET_ACCESS_KEY'] = os.environ.get('SEAWEEDFS_PASSWORD', 'Zh5EvRHAivHggdI85V4TwqG4jCJLlWEo')

# Get model from MLflow
try:
    client = mlflow.MlflowClient()
    model_name = '{{ mlflow_model }}'

    # Get latest version
    versions = client.search_model_versions(f\"name='{model_name}'\")
    if not versions:
        print(f'ERROR: Model {model_name} not found in MLflow', file=sys.stderr)
        sys.exit(1)

    latest = max(versions, key=lambda v: int(v.version))

    # Extract S3 path from source (format: runs:/run_id/model)
    source = latest.source
    if source.startswith('runs:/'):
        run_id = source.split('/')[1]
        # MLflow stores at: s3://mlflow/artifacts/run_id/artifacts/model
        s3_path = f's3://mlflow/artifacts/{run_id}/artifacts/model'
    else:
        print(f'ERROR: Unexpected source format: {source}', file=sys.stderr)
        sys.exit(1)

    print(s3_path)

except Exception as e:
    print(f'ERROR: Failed to query MLflow: {e}', file=sys.stderr)
    sys.exit(1)
")

if [ $? -ne 0 ] || [ -z "$MODEL_S3_PATH" ]; then
    echo "ERROR: Failed to get model S3 path from MLflow"
    exit 1
fi

echo "✓ Model S3 Path: $MODEL_S3_PATH"

# Set S3 credentials for TensorRT-LLM
export AWS_ACCESS_KEY_ID="seaweedfs"
export AWS_SECRET_ACCESS_KEY="${SEAWEEDFS_PASSWORD:-Zh5EvRHAivHggdI85V4TwqG4jCJLlWEo}"
export AWS_ENDPOINT_URL="http://seaweedfs-filer.seaweedfs.svc.cluster.local:8333"
export AWS_DEFAULT_REGION="us-east-1"

# Create TensorRT-LLM API config
cat > /tmp/extra-llm-api-config.yml <<EOF
print_iter_log: false
kv_cache_config:
  dtype: "auto"
  free_gpu_memory_fraction: 0.9
cuda_graph_config:
  enable_padding: true
disable_overlap_scheduler: true
EOF

# Start TensorRT-LLM server in background (loading directly from S3!)
echo "Starting TensorRT-LLM server (loading from S3)..."
trtllm-serve "$MODEL_S3_PATH" \
  --max_batch_size 64 \
  --trust_remote_code \
  --port 8355 \
  --host 0.0.0.0 \
  --extra_llm_api_options /tmp/extra-llm-api-config.yml &

# Wait for server to be ready
echo "Waiting for TensorRT-LLM server to start..."
for i in {1..120}; do
    if curl -s http://localhost:8355/v1/models >/dev/null 2>&1; then
        echo "✓ TensorRT-LLM server is ready"
        break
    fi
    if [ $i -eq 120 ]; then
        echo "ERROR: TensorRT-LLM server failed to start (timeout)"
        exit 1
    fi
    sleep 2
done

# Start Gradio UI
echo "Starting Gradio UI on port 7860..."
exec python3 server.py
