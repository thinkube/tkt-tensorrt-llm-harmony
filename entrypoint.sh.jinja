#!/bin/bash
set -e

echo "=== TensorRT-LLM Inference Server Startup ===\"
echo "Model: {{ model_id }}"

# Configure HuggingFace to use the shared JuiceFS cache
# The model mirroring workflow created persistent symlinks at:
# /mlflow-models/.cache/huggingface/hub/models--{org}--{model}/snapshots/main
export HF_HOME="/mlflow-models/.cache"
export HF_HUB_CACHE="/mlflow-models/.cache/huggingface/hub"

echo "Using shared HuggingFace cache on JuiceFS: $HF_HUB_CACHE"

# Create TensorRT-LLM API config
cat > /tmp/extra-llm-api-config.yml <<EOF
print_iter_log: false
kv_cache_config:
  dtype: "auto"
  free_gpu_memory_fraction: 0.9
cuda_graph_config:
  enable_padding: true
disable_overlap_scheduler: true
EOF

# Start TensorRT-LLM server
# Model is loaded from JuiceFS via persistent HF cache symlink - no download needed!
echo "Starting TensorRT-LLM server..."
trtllm-serve "{{ model_id }}" \
  --backend pytorch \
  --max_batch_size 64 \
  --trust_remote_code \
  --port 8355 \
  --host 0.0.0.0 \
  --extra_llm_api_options /tmp/extra-llm-api-config.yml &

# Wait for server to be ready (30 minutes for large models)
echo "Waiting for TensorRT-LLM server to start..."
for i in {1..900}; do
    if curl -s http://localhost:8355/v1/models >/dev/null 2>&1; then
        echo "âœ“ TensorRT-LLM server is ready"
        break
    fi
    if [ $i -eq 900 ]; then
        echo "ERROR: TensorRT-LLM server failed to start (timeout after 30 minutes)"
        exit 1
    fi
    sleep 2
done

# Start Gradio UI
echo "Starting Gradio UI on port 7860..."
exec python3 server.py
