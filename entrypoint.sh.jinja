#!/bin/bash
set -e

echo "=== TensorRT-LLM Inference Server Startup ==="
echo "Model: {{ model_id }}"

# Download pre-quantized model from HuggingFace
echo "Downloading pre-quantized model from HuggingFace..."
hf download "{{ model_id }}"

# Create TensorRT-LLM API config following NVIDIA single-node serving pattern
cat > /tmp/extra-llm-api-config.yml <<EOF
print_iter_log: false
kv_cache_config:
  dtype: "auto"
  free_gpu_memory_fraction: 0.9
cuda_graph_config:
  enable_padding: true
disable_overlap_scheduler: true
EOF

# Start TensorRT-LLM server in background
echo "Starting TensorRT-LLM OpenAI-compatible server on port 8355..."
trtllm-serve "{{ model_id }}" \
  --max_batch_size 64 \
  --trust_remote_code \
  --port 8355 \
  --host 0.0.0.0 \
  --extra_llm_api_options /tmp/extra-llm-api-config.yml &

# Wait for server to be ready
echo "Waiting for TensorRT-LLM server to start..."
for i in {1..60}; do
    if curl -s http://localhost:8355/v1/models >/dev/null 2>&1; then
        echo "TensorRT-LLM server is ready"
        break
    fi
    if [ $i -eq 60 ]; then
        echo "ERROR: TensorRT-LLM server failed to start"
        exit 1
    fi
    sleep 2
done

# Start Gradio UI
echo "Starting Gradio UI on port 7860..."
exec python3 server.py
