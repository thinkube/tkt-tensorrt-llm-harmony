apiVersion: thinkube.io/v1
kind: TemplateManifest
metadata:
  name: tkt-tensorrt-llm
  title: TensorRT-LLM Inference Server (MLflow)
  description: NVIDIA TensorRT-LLM optimized inference loading models directly from MLflow Model Registry
  version: 1.0.0
  author: Alejandro Martínez Corriá
  tags: ["ai", "llm", "tensorrt-llm", "nvidia", "gradio", "inference", "text-generation", "gpu", "mlflow"]

parameters:
  - name: mlflow_model
    type: str
    description: MLflow model name from Model Registry (e.g., nvidia-Phi-4-multimodal-instruct-FP4)
    default: "nvidia-Phi-4-multimodal-instruct-FP4"
    pattern: "^[a-zA-Z0-9_-]+$"
    
