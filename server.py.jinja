#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with NVFP4 support on Blackwell (DGX Spark GB10)
Uses TensorRT-LLM Python API for direct model loading
"""

import os
import json
import time
import gradio as gr
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from tensorrt_llm import LLM, SamplingParams
from transformers import AutoTokenizer
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Model configuration from template parameter
MODEL_ID = "{{ model_id }}"
MODEL_PATH = os.environ.get('MODEL_PATH')

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# Global LLM and tokenizer instances (initialized in main block to support MPI)
llm = None
tokenizer = None

def initialize_model():
    """Initialize TensorRT-LLM model and tokenizer (must be called from main block for MPI support)"""
    global llm, tokenizer
    print(f"Loading model from: {MODEL_PATH}")
    print(f"Model ID: {MODEL_ID}")

    # Load tokenizer for chat template formatting
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    # Load TensorRT-LLM model
    print("Loading TensorRT-LLM model...")
    llm = LLM(model=MODEL_PATH)

    print(f"âœ“ TensorRT-LLM model and tokenizer loaded successfully")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response using TensorRT-LLM Python API with harmony chat template"""
    # Convert Gradio history to OpenAI messages format
    messages = []
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})

    # Add current message
    messages.append({"role": "user", "content": message})

    # Apply chat template to format messages with harmony format
    # This adds the special tokens like <|start|>, <|message|>, <|channel|>, etc.
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Configure sampling parameters
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=0.9
    )

    # Generate with TensorRT-LLM Python API
    outputs = llm.generate([prompt], sampling_params)

    # Extract and parse response (remove harmony format reasoning)
    raw_response = outputs[0].outputs[0].text
    parsed = parse_harmony_response(raw_response)
    yield parsed["final"]

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if model is loaded
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "model_path": MODEL_PATH,
            "engine": "TensorRT-LLM (Python API)"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


# ============================================================================
# OpenAI-Compatible API Endpoints (for LiteLLM integration)
# ============================================================================

import re

def parse_harmony_response(text: str) -> dict:
    """
    Parse harmony format response to extract different channels.
    Harmony format uses channels: analysis (reasoning), commentary (tools), final (response)
    Returns dict with 'final' (user-facing) and 'reasoning' (chain-of-thought) content.
    """
    result = {"final": text, "reasoning": None}

    # Pattern: look for 'final' or 'assistantfinal' marker followed by content
    # The format concatenates: "analysis<reasoning>assistantfinal<answer>" or similar
    final_patterns = [
        r'assistantfinal(.*)$',  # assistantfinal followed by content
        r'(?:^|[^a-z])final(.*)$',  # final at word boundary followed by content
    ]

    for pattern in final_patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            final_content = match.group(1).strip()
            if final_content:
                result["final"] = final_content
                # Extract reasoning (everything before the final marker)
                reasoning = text[:match.start()].strip()
                # Remove 'analysis' prefix if present
                reasoning = re.sub(r'^analysis\s*', '', reasoning, flags=re.IGNORECASE)
                if reasoning:
                    result["reasoning"] = reasoning
                break

    return result


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAI-compatible chat completions endpoint using TensorRT-LLM"""
    try:
        body = await request.json()
        messages = body.get('messages', [])
        temperature = body.get('temperature', 0.7)
        max_tokens = body.get('max_tokens', 512)
        stream = body.get('stream', False)
        top_p = body.get('top_p', 0.9)
        include_reasoning = body.get('include_reasoning', False)  # Optional: include chain-of-thought

        # Apply chat template (uses harmony format for gpt-oss, chatml for others, etc.)
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Configure sampling parameters
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p
        )

        # Generate with TensorRT-LLM
        outputs = llm.generate([prompt], sampling_params)
        raw_response = outputs[0].outputs[0].text

        # Parse harmony format to extract final response
        parsed = parse_harmony_response(raw_response)
        response_text = parsed["final"]

        # Optionally include reasoning in a separate field
        reasoning_text = parsed["reasoning"] if include_reasoning else None

        # Generate unique ID
        completion_id = f"chatcmpl-{int(time.time() * 1000)}"
        created = int(time.time())

        if stream:
            # Streaming response (simplified - sends full response in one chunk)
            async def generate_stream():
                chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {"role": "assistant", "content": response_text},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"

                # Final chunk with finish_reason
                final_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )

        # Non-streaming response
        response_data = {
            "id": completion_id,
            "object": "chat.completion",
            "created": created,
            "model": MODEL_ID,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,  # TensorRT-LLM doesn't expose token counts easily
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }

        # Optionally include reasoning (chain-of-thought) in response
        if reasoning_text:
            response_data["reasoning"] = reasoning_text

        return JSONResponse(response_data)

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


@app.get("/v1/models")
async def openai_models():
    """OpenAI-compatible models endpoint - returns available LLM model"""
    return JSONResponse({
        "object": "list",
        "data": [{
            "id": MODEL_ID,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "thinkube",
            "permission": [],
            "root": MODEL_ID,
            "parent": None
        }]
    })


# Mount Gradio app with Thinkube favicon
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Initialize model (must be in main block for MPI support)
    initialize_model()

    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
