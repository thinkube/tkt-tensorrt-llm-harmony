#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with NVFP4 support on Blackwell (DGX Spark GB10)
Uses TensorRT-LLM Python API for direct model loading
"""

import os
import json
import time
import logging
import gradio as gr
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from tensorrt_llm import LLM, SamplingParams
from transformers import AutoTokenizer
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Model configuration from template parameter
MODEL_ID = "{{ model_id }}"
MODEL_PATH = os.environ.get('MODEL_PATH')

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# Global LLM and tokenizer instances (initialized in main block to support MPI)
llm = None
tokenizer = None

def initialize_model():
    """Initialize TensorRT-LLM model and tokenizer (must be called from main block for MPI support)"""
    global llm, tokenizer
    print(f"Loading model from: {MODEL_PATH}")
    print(f"Model ID: {MODEL_ID}")

    # Load tokenizer for chat template formatting
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    # Load TensorRT-LLM model
    print("Loading TensorRT-LLM model...")
    llm = LLM(model=MODEL_PATH)

    print(f"âœ“ TensorRT-LLM model and tokenizer loaded successfully")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response using TensorRT-LLM Python API with harmony chat template"""
    # Convert Gradio history to OpenAI messages format
    messages = []
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})

    # Add current message
    messages.append({"role": "user", "content": message})

    # Apply chat template to format messages with harmony format
    # This adds the special tokens like <|start|>, <|message|>, <|channel|>, etc.
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Configure sampling parameters
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=0.9
    )

    # Generate with TensorRT-LLM Python API
    outputs = llm.generate([prompt], sampling_params)

    # Extract and parse response (remove harmony format reasoning)
    raw_response = outputs[0].outputs[0].text
    parsed = parse_harmony_response(raw_response)
    yield parsed["final"]

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if model is loaded
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "model_path": MODEL_PATH,
            "engine": "TensorRT-LLM (Python API)"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


# ============================================================================
# OpenAI-Compatible API Endpoints (for LiteLLM integration)
# ============================================================================

import re

def parse_harmony_response(text: str) -> dict:
    """
    Parse harmony format response to extract different channels.
    Harmony format uses specific markers like 'assistantfinal' (no spaces).
    Returns dict with 'final' (user-facing) and 'reasoning' (chain-of-thought) content.

    Only matches exact harmony format markers to avoid truncating normal responses
    that contain common words like 'final'.
    """
    result = {"final": text, "reasoning": None}

    # Only match the specific harmony format marker: assistantfinal (one word, no spaces)
    # This marker appears at the boundary between reasoning and final response
    match = re.search(r'assistantfinal(.*)$', text, re.IGNORECASE | re.DOTALL)
    if match:
        final_content = match.group(1).strip()
        if final_content:
            result["final"] = final_content
            # Extract reasoning (everything before the marker)
            reasoning = text[:match.start()].strip()
            # Remove 'analysis' prefix if present
            reasoning = re.sub(r'^analysis\s*', '', reasoning, flags=re.IGNORECASE)
            if reasoning:
                result["reasoning"] = reasoning

    return result


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAI-compatible chat completions endpoint using TensorRT-LLM"""
    try:
        body = await request.json()
        messages = body.get('messages', [])
        temperature = body.get('temperature', 0.7)
        max_tokens = body.get('max_tokens', 512)
        stream = body.get('stream', False)
        top_p = body.get('top_p', 0.9)
        include_reasoning = body.get('include_reasoning', False)  # Optional: include chain-of-thought

        # Apply chat template (uses harmony format for gpt-oss, chatml for others, etc.)
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        logger.info(f"=== API Request ===")
        logger.info(f"max_tokens: {max_tokens}, temperature: {temperature}, top_p: {top_p}")
        logger.info(f"Prompt length (chars): {len(prompt)}")

        # Configure sampling parameters
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p
        )

        # Generate with TensorRT-LLM (async to handle concurrent requests)
        output = await llm.generate_async(prompt, sampling_params)
        raw_response = output.outputs[0].text
        finish_reason = getattr(output.outputs[0], 'finish_reason', 'unknown')

        logger.info(f"=== Raw Response ===")
        logger.info(f"Length (chars): {len(raw_response)}")
        logger.info(f"Finish reason: {finish_reason}")
        logger.info(f"RAW_RESPONSE_START>>>{raw_response}<<<RAW_RESPONSE_END")

        # Parse harmony format to extract final response
        parsed = parse_harmony_response(raw_response)
        response_text = parsed["final"]

        logger.info(f"=== Parsed Response ===")
        logger.info(f"Harmony parsing applied: {parsed['reasoning'] is not None}")
        logger.info(f"Final length (chars): {len(response_text)}")
        logger.info(f"PARSED_RESPONSE_START>>>{response_text}<<<PARSED_RESPONSE_END")

        # Optionally include reasoning in a separate field
        reasoning_text = parsed["reasoning"] if include_reasoning else None

        # Generate unique ID
        completion_id = f"chatcmpl-{int(time.time() * 1000)}"
        created = int(time.time())

        if stream:
            # Streaming response (simplified - sends full response in one chunk)
            async def generate_stream():
                chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {"role": "assistant", "content": response_text},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"

                # Final chunk with finish_reason
                final_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )

        # Non-streaming response
        response_data = {
            "id": completion_id,
            "object": "chat.completion",
            "created": created,
            "model": MODEL_ID,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,  # TensorRT-LLM doesn't expose token counts easily
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }

        # Optionally include reasoning (chain-of-thought) in response
        if reasoning_text:
            response_data["reasoning"] = reasoning_text

        return JSONResponse(response_data)

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


@app.get("/v1/models")
async def openai_models():
    """OpenAI-compatible models endpoint - returns available LLM model"""
    return JSONResponse({
        "object": "list",
        "data": [{
            "id": MODEL_ID,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "thinkube",
            "permission": [],
            "root": MODEL_ID,
            "parent": None
        }]
    })


# ============================================================================
# Batch Completions Endpoint (for parallel data generation)
# ============================================================================

# DGX Spark unified memory (128GB): 20B NVFP4 model uses ~15GB, leaving ~100GB for KV cache
# With 80Gi pod memory limit, batch size 8 is safe for 800 max_tokens
MAX_BATCH_SIZE = 8

@app.post("/v1/batch/completions")
async def batch_completions(request: Request):
    """
    Batch completions endpoint for parallel inference.

    Accepts multiple prompts and processes them in GPU batches,
    providing significant speedup over sequential requests.

    Large batches are automatically chunked to prevent OOM on unified memory.

    Request format:
    {
        "requests": [
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            ...
        ]
    }

    Response format:
    {
        "responses": [
            {"content": "...", "finish_reason": "stop"},
            {"content": "...", "finish_reason": "stop"},
            ...
        ],
        "batch_size": N,
        "processing_time_ms": X,
        "chunks_processed": Y
    }
    """
    import gc
    import torch

    try:
        start_time = time.time()
        body = await request.json()
        requests_list = body.get('requests', [])

        if not requests_list:
            return JSONResponse(
                status_code=400,
                content={"error": {"message": "No requests provided", "type": "invalid_request"}}
            )

        total_requests = len(requests_list)
        logger.info(f"=== Batch Request === total={total_requests}, max_chunk={MAX_BATCH_SIZE}")

        # Prepare all prompts and sampling params
        all_prompts = []
        all_params = []

        for req in requests_list:
            messages = req.get('messages', [])
            temperature = req.get('temperature', 0.7)
            max_tokens = req.get('max_tokens', 512)
            top_p = req.get('top_p', 0.9)

            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            all_prompts.append(prompt)
            all_params.append(SamplingParams(
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p
            ))

        # Process in chunks to prevent OOM
        all_responses = []
        chunks_processed = 0

        for i in range(0, total_requests, MAX_BATCH_SIZE):
            chunk_prompts = all_prompts[i:i + MAX_BATCH_SIZE]
            chunk_params = all_params[i:i + MAX_BATCH_SIZE]
            chunk_size = len(chunk_prompts)

            logger.info(f"Processing chunk {chunks_processed + 1}: {chunk_size} prompts (indices {i}-{i + chunk_size - 1})")

            # Use first request's params for this chunk
            batch_sampling_params = chunk_params[0]

            # Generate this chunk
            outputs = llm.generate(chunk_prompts, batch_sampling_params)

            # Process responses
            for output in outputs:
                raw_response = output.outputs[0].text
                finish_reason = getattr(output.outputs[0], 'finish_reason', 'stop')
                parsed = parse_harmony_response(raw_response)

                all_responses.append({
                    "content": parsed["final"],
                    "finish_reason": str(finish_reason) if finish_reason else "stop",
                    "reasoning": parsed.get("reasoning")
                })

            chunks_processed += 1

            # Memory cleanup between chunks to prevent accumulation
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        processing_time_ms = int((time.time() - start_time) * 1000)
        logger.info(f"Batch completed: {total_requests} prompts in {chunks_processed} chunks, {processing_time_ms}ms total ({processing_time_ms/total_requests:.0f}ms/prompt)")

        return JSONResponse({
            "responses": all_responses,
            "batch_size": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests,
            "chunks_processed": chunks_processed,
            "max_chunk_size": MAX_BATCH_SIZE
        })

    except Exception as e:
        logger.error(f"Batch error: {e}")
        # Memory cleanup on error
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


# Mount Gradio app with Thinkube favicon
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Initialize model (must be in main block for MPI support)
    initialize_model()

    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
