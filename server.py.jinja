#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with NVFP4 support on Blackwell (DGX Spark GB10)
Uses TensorRT-LLM Python API for direct model loading
"""

import os
import json
import time
import logging
import gradio as gr
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from tensorrt_llm import LLM, SamplingParams
from transformers import AutoTokenizer
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Official OpenAI harmony format parsing
from openai_harmony import load_harmony_encoding, HarmonyEncodingName, Role

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Model configuration from template parameter
MODEL_ID = "{{ model_id }}"
MODEL_PATH = os.environ.get('MODEL_PATH')

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# Global LLM and tokenizer instances (initialized in main block to support MPI)
llm = None
tokenizer = None

def initialize_model():
    """Initialize TensorRT-LLM model and tokenizer (must be called from main block for MPI support)"""
    global llm, tokenizer
    print(f"Loading model from: {MODEL_PATH}")
    print(f"Model ID: {MODEL_ID}")

    # Load tokenizer for chat template formatting
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    # Load TensorRT-LLM model
    print("Loading TensorRT-LLM model...")
    llm = LLM(model=MODEL_PATH)

    print(f"âœ“ TensorRT-LLM model and tokenizer loaded successfully")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response using TensorRT-LLM Python API with harmony chat template"""
    # Convert Gradio history to OpenAI messages format
    messages = []
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})

    # Add current message
    messages.append({"role": "user", "content": message})

    # Apply chat template to format messages with harmony format
    # This adds the special tokens like <|start|>, <|message|>, <|channel|>, etc.
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Configure sampling parameters
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=0.9
    )

    # Generate with TensorRT-LLM Python API
    outputs = llm.generate([prompt], sampling_params)

    # Extract and parse response (remove harmony format reasoning)
    raw_response = outputs[0].outputs[0].text
    parsed = parse_harmony_response(raw_response)
    yield parsed["final"]

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if model is loaded
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "model_path": MODEL_PATH,
            "engine": "TensorRT-LLM (Python API)"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


# ============================================================================
# OpenAI-Compatible API Endpoints (for LiteLLM integration)
# ============================================================================

# Initialize harmony encoding
_harmony_enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)


def parse_harmony_response(text: str) -> dict:
    """
    Parse harmony format response to extract different channels.
    Uses the official openai-harmony package.
    Returns dict with 'final' (user-facing) and 'reasoning' (chain-of-thought) content.
    """
    result = {"final": text, "reasoning": None}

    try:
        # Tokenize the text and parse
        tokens = _harmony_enc.encode(text)
        messages = _harmony_enc.parse_messages_from_completion_tokens(
            tokens, role=Role.ASSISTANT, strict=False
        )

        # Extract content by channel
        final_parts = []
        reasoning_parts = []

        for msg in messages:
            channel = getattr(msg, 'channel', None)
            content = getattr(msg, 'content', '')

            if channel == 'final':
                final_parts.append(content)
            elif channel == 'analysis':
                reasoning_parts.append(content)
            elif channel is None or channel == '':
                # Default channel - treat as final
                final_parts.append(content)

        if final_parts:
            result["final"] = ''.join(final_parts)
        if reasoning_parts:
            result["reasoning"] = ''.join(reasoning_parts)

    except Exception as e:
        logger.warning(f"Harmony parsing failed: {e}")

    return result


async def _handle_batch_completions(body: dict, batch_requests: list):
    """
    Handle batch completions with true GPU batching.

    This function processes multiple requests in a single GPU batch operation,
    providing significant speedup over sequential processing.

    Request format (via /v1/chat/completions):
    {
        "batch": [
            {"messages": [...]},
            {"messages": [...]}
        ],
        "max_tokens": 512,      # Shared defaults (can be overridden per request)
        "temperature": 0.7,
        "top_p": 0.9
    }

    Response format:
    {
        "object": "batch.completion",
        "choices": [
            {"index": 0, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"},
            {"index": 1, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"}
        ],
        "usage": {...},
        "batch_info": {"count": N, "processing_time_ms": X}
    }
    """
    import gc
    import asyncio
    from concurrent.futures import ThreadPoolExecutor

    start_time = time.time()

    # Shared defaults from outer request
    default_temperature = body.get('temperature', 0.7)
    default_max_tokens = body.get('max_tokens', 512)
    default_top_p = body.get('top_p', 0.9)
    include_reasoning = body.get('include_reasoning', False)

    total_requests = len(batch_requests)
    logger.info(f"=== Batch Request via /v1/chat/completions === total={total_requests}")

    if total_requests == 0:
        return JSONResponse(
            status_code=400,
            content={"error": {"message": "Empty batch", "type": "invalid_request"}}
        )

    # Prepare all prompts
    all_prompts = []
    for req in batch_requests:
        messages = req.get('messages', [])
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        all_prompts.append(prompt)

    # Use first request's params (or defaults) for batch
    first_req = batch_requests[0]
    sampling_params = SamplingParams(
        temperature=first_req.get('temperature', default_temperature),
        max_tokens=first_req.get('max_tokens', default_max_tokens),
        top_p=first_req.get('top_p', default_top_p)
    )

    # Process in chunks to prevent OOM (same as /v1/batch/completions)
    all_choices = []
    chunks_processed = 0

    # Thread pool for running sync generate without blocking event loop
    executor = ThreadPoolExecutor(max_workers=1)

    def sync_generate_batch(prompts, params):
        """Run sync generate in thread to preserve true GPU batching"""
        return llm.generate(prompts, params)

    for i in range(0, total_requests, MAX_BATCH_SIZE):
        chunk_prompts = all_prompts[i:i + MAX_BATCH_SIZE]
        chunk_size = len(chunk_prompts)

        logger.info(f"Processing batch chunk {chunks_processed + 1}: {chunk_size} prompts")

        # Run true GPU batch in thread pool
        loop = asyncio.get_event_loop()
        outputs = await loop.run_in_executor(
            executor,
            sync_generate_batch,
            chunk_prompts,
            sampling_params
        )

        # Process responses
        for j, output in enumerate(outputs):
            raw_response = output.outputs[0].text
            finish_reason = getattr(output.outputs[0], 'finish_reason', 'stop')
            parsed = parse_harmony_response(raw_response)

            choice = {
                "index": i + j,
                "message": {
                    "role": "assistant",
                    "content": parsed["final"]
                },
                "finish_reason": str(finish_reason) if finish_reason else "stop"
            }

            # Optionally include reasoning
            if include_reasoning and parsed.get("reasoning"):
                choice["reasoning"] = parsed["reasoning"]

            all_choices.append(choice)

        chunks_processed += 1

        # Memory cleanup between chunks
        try:
            gc.collect()
        except Exception:
            pass

    processing_time_ms = int((time.time() - start_time) * 1000)
    logger.info(f"Batch completed: {total_requests} prompts, {processing_time_ms}ms ({processing_time_ms/total_requests:.0f}ms/prompt)")

    # Return OpenAI-like response with batch info
    return JSONResponse({
        "id": f"batch-{int(time.time() * 1000)}",
        "object": "batch.completion",
        "created": int(time.time()),
        "model": MODEL_ID,
        "choices": all_choices,
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        },
        "batch_info": {
            "count": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests,
            "chunks_processed": chunks_processed
        }
    })


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAI-compatible chat completions endpoint using TensorRT-LLM

    Supports both single and batch requests:
    - Single: {"messages": [...], "max_tokens": 512, ...}
    - Batch:  {"batch": [{"messages": [...]}, {"messages": [...]}], "max_tokens": 512, ...}

    Batch mode uses true GPU batching for maximum throughput.
    This allows batch requests to flow through LiteLLM unchanged.
    """
    try:
        body = await request.json()

        # Check for batch mode - array of requests in "batch" field
        batch_requests = body.get('batch')
        if batch_requests and isinstance(batch_requests, list):
            return await _handle_batch_completions(body, batch_requests)

        # Single request mode (standard OpenAI format)
        messages = body.get('messages', [])
        temperature = body.get('temperature', 0.7)
        max_tokens = body.get('max_tokens', 512)
        stream = body.get('stream', False)
        top_p = body.get('top_p', 0.9)
        include_reasoning = body.get('include_reasoning', False)  # Optional: include chain-of-thought

        # Apply chat template (uses harmony format for gpt-oss, chatml for others, etc.)
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        logger.info(f"=== API Request ===")
        logger.info(f"max_tokens: {max_tokens}, temperature: {temperature}, top_p: {top_p}")
        logger.info(f"Prompt length (chars): {len(prompt)}")

        # Configure sampling parameters
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p
        )

        # Generate with TensorRT-LLM (async to handle concurrent requests)
        output = await llm.generate_async(prompt, sampling_params)
        raw_response = output.outputs[0].text
        finish_reason = getattr(output.outputs[0], 'finish_reason', 'unknown')

        logger.info(f"=== Raw Response ===")
        logger.info(f"Length (chars): {len(raw_response)}")
        logger.info(f"Finish reason: {finish_reason}")
        logger.info(f"RAW_RESPONSE_START>>>{raw_response}<<<RAW_RESPONSE_END")

        # Parse harmony format to extract final response
        parsed = parse_harmony_response(raw_response)
        response_text = parsed["final"]

        logger.info(f"=== Parsed Response ===")
        logger.info(f"Harmony parsing applied: {parsed['reasoning'] is not None}")
        logger.info(f"Final length (chars): {len(response_text)}")
        logger.info(f"PARSED_RESPONSE_START>>>{response_text}<<<PARSED_RESPONSE_END")

        # Optionally include reasoning in a separate field
        reasoning_text = parsed["reasoning"] if include_reasoning else None

        # Generate unique ID
        completion_id = f"chatcmpl-{int(time.time() * 1000)}"
        created = int(time.time())

        if stream:
            # Streaming response (simplified - sends full response in one chunk)
            async def generate_stream():
                chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {"role": "assistant", "content": response_text},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"

                # Final chunk with finish_reason
                final_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )

        # Non-streaming response
        response_data = {
            "id": completion_id,
            "object": "chat.completion",
            "created": created,
            "model": MODEL_ID,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,  # TensorRT-LLM doesn't expose token counts easily
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }

        # Optionally include reasoning (chain-of-thought) in response
        if reasoning_text:
            response_data["reasoning"] = reasoning_text

        return JSONResponse(response_data)

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


@app.get("/v1/models")
async def openai_models():
    """OpenAI-compatible models endpoint - returns available LLM model"""
    return JSONResponse({
        "object": "list",
        "data": [{
            "id": MODEL_ID,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "thinkube",
            "permission": [],
            "root": MODEL_ID,
            "parent": None
        }]
    })


# ============================================================================
# Batch Completions Endpoint (for parallel data generation)
# ============================================================================

# DGX Spark unified memory (128GB): 20B NVFP4 model uses ~15GB, leaving ~100GB for KV cache
# With 80Gi pod memory limit, batch size 8 is safe for 800 max_tokens
MAX_BATCH_SIZE = 8

@app.post("/v1/batch/completions")
async def batch_completions(request: Request):
    """
    Batch completions endpoint for parallel inference.

    Accepts multiple prompts and processes them in GPU batches,
    providing significant speedup over sequential requests.

    Large batches are automatically chunked to prevent OOM on unified memory.

    Request format:
    {
        "requests": [
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            ...
        ]
    }

    Response format:
    {
        "responses": [
            {"content": "...", "finish_reason": "stop"},
            {"content": "...", "finish_reason": "stop"},
            ...
        ],
        "batch_size": N,
        "processing_time_ms": X,
        "chunks_processed": Y
    }
    """
    import gc

    try:
        start_time = time.time()
        body = await request.json()
        requests_list = body.get('requests', [])

        if not requests_list:
            return JSONResponse(
                status_code=400,
                content={"error": {"message": "No requests provided", "type": "invalid_request"}}
            )

        total_requests = len(requests_list)
        logger.info(f"=== Batch Request === total={total_requests}, max_chunk={MAX_BATCH_SIZE}")

        # Prepare all prompts and sampling params
        all_prompts = []
        all_params = []

        for req in requests_list:
            messages = req.get('messages', [])
            temperature = req.get('temperature', 0.7)
            max_tokens = req.get('max_tokens', 512)
            top_p = req.get('top_p', 0.9)

            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            all_prompts.append(prompt)
            all_params.append(SamplingParams(
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p
            ))

        # Process in chunks to prevent OOM
        all_responses = []
        chunks_processed = 0

        import asyncio
        from concurrent.futures import ThreadPoolExecutor

        # Use thread pool to run sync generate() without blocking event loop
        # This preserves true GPU batching while being async-compatible
        executor = ThreadPoolExecutor(max_workers=1)

        def sync_generate_batch(prompts, params):
            """Run sync generate in thread to avoid blocking uvicorn event loop"""
            return llm.generate(prompts, params)

        for i in range(0, total_requests, MAX_BATCH_SIZE):
            chunk_prompts = all_prompts[i:i + MAX_BATCH_SIZE]
            chunk_params = all_params[i:i + MAX_BATCH_SIZE]
            chunk_size = len(chunk_prompts)

            logger.info(f"Processing chunk {chunks_processed + 1}: {chunk_size} prompts (indices {i}-{i + chunk_size - 1})")

            # Use first request's params for this chunk (TensorRT-LLM batches internally)
            batch_sampling_params = chunk_params[0]

            # Run sync generate in thread pool to preserve true GPU batching
            # while not blocking the uvicorn event loop
            loop = asyncio.get_event_loop()
            outputs = await loop.run_in_executor(
                executor,
                sync_generate_batch,
                chunk_prompts,
                batch_sampling_params
            )

            # Process responses
            for output in outputs:
                raw_response = output.outputs[0].text
                finish_reason = getattr(output.outputs[0], 'finish_reason', 'stop')
                parsed = parse_harmony_response(raw_response)

                all_responses.append({
                    "content": parsed["final"],
                    "finish_reason": str(finish_reason) if finish_reason else "stop",
                    "reasoning": parsed.get("reasoning")
                })

            chunks_processed += 1

            # Memory cleanup between chunks (wrapped in try/except for TensorRT-LLM compatibility)
            try:
                gc.collect()
            except Exception as cleanup_error:
                logger.warning(f"Memory cleanup warning: {cleanup_error}")

        processing_time_ms = int((time.time() - start_time) * 1000)
        logger.info(f"Batch completed: {total_requests} prompts in {chunks_processed} chunks, {processing_time_ms}ms total ({processing_time_ms/total_requests:.0f}ms/prompt)")

        return JSONResponse({
            "responses": all_responses,
            "batch_size": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests,
            "chunks_processed": chunks_processed,
            "max_chunk_size": MAX_BATCH_SIZE
        })

    except Exception as e:
        logger.error(f"Batch error: {e}")
        # Memory cleanup on error
        try:
            gc.collect()
        except:
            pass
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


# Mount Gradio app with Thinkube favicon
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Initialize model (must be in main block for MPI support)
    initialize_model()

    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
