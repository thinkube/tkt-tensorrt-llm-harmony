#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with NVFP4 support on Blackwell (DGX Spark GB10)
Uses TensorRT-LLM Python API for direct model loading
"""

import os
import json
import time
import logging
import gradio as gr
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from tensorrt_llm import LLM, SamplingParams
from transformers import AutoTokenizer
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Official OpenAI harmony format parsing
from openai_harmony import load_harmony_encoding, HarmonyEncodingName, Role

# Harmony format stop tokens (from openai-harmony spec)
# These tokens indicate model should stop generating:
# - <|return|> (200002): Model is done with final response
# - <|call|> (200012): Model wants to call a tool
# The model's generation_config.json should include these as eos_token_id,
# but we explicitly pass them to ensure TensorRT-LLM stops generation.
HARMONY_STOP_TOKEN_IDS = [200002, 200012]

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Model configuration from template parameter
MODEL_ID = "{{ model_id }}"
MODEL_PATH = os.environ.get('MODEL_PATH')

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# Global LLM and tokenizer instances (initialized in main block to support MPI)
llm = None
tokenizer = None

def initialize_model():
    """Initialize TensorRT-LLM model and tokenizer (must be called from main block for MPI support)"""
    global llm, tokenizer
    print(f"Loading model from: {MODEL_PATH}")
    print(f"Model ID: {MODEL_ID}")

    # Load tokenizer for chat template formatting
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    # Load TensorRT-LLM model
    print("Loading TensorRT-LLM model...")
    llm = LLM(model=MODEL_PATH)

    print(f"âœ“ TensorRT-LLM model and tokenizer loaded successfully")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response using TensorRT-LLM Python API with harmony chat template"""
    # Convert Gradio history to OpenAI messages format
    messages = []
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})

    # Add current message
    messages.append({"role": "user", "content": message})

    # Apply chat template to format messages with harmony format
    # This adds the special tokens like <|start|>, <|message|>, <|channel|>, etc.
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Configure sampling parameters
    # skip_special_tokens=False preserves harmony format tokens for parsing
    # stop_token_ids ensures model stops at <|return|> or <|call|> tokens
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=0.9,
        skip_special_tokens=False,
        stop_token_ids=HARMONY_STOP_TOKEN_IDS
    )

    # Generate with TensorRT-LLM Python API
    outputs = llm.generate([prompt], sampling_params)

    # Extract and parse response (remove harmony format reasoning)
    raw_text = outputs[0].outputs[0].text
    parsed = parse_harmony_response(raw_text)
    yield parsed["final"]

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if model is loaded
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "model_path": MODEL_PATH,
            "engine": "TensorRT-LLM (Python API)"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


# ============================================================================
# OpenAI-Compatible API Endpoints (for LiteLLM integration)
# ============================================================================

# Initialize harmony encoding
_harmony_enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)


# ============================================================================
# Tool Calling Support (OpenAI-compatible tools -> Harmony format)
# ============================================================================
{% raw %}
def convert_tools_to_harmony_namespace(tools: list) -> str:
    """
    Convert OpenAI tools format to harmony namespace TypeScript-style definition.

    Input (OpenAI format):
    [{"type": "function", "function": {
        "name": "search_papers",
        "description": "Search papers",
        "parameters": {"type": "object", "properties": {"query": {"type": "string"}}}
    }}]

    Output (Harmony format):
    # Tools
    ## functions
    namespace functions {
      // Search papers
      type search_papers = (_: {
        query: string,
      }) => any;
    }
    """
    if not tools:
        return ""

    lines = ["# Tools", "## functions", "namespace functions {"]

    for tool in tools:
        if tool.get("type") != "function":
            continue
        func = tool.get("function", {})
        name = func.get("name", "unknown")
        desc = func.get("description", "")
        params = func.get("parameters", {})

        # Add description as comment
        if desc:
            lines.append(f"  // {desc}")

        # Convert parameters to TypeScript-style
        props = params.get("properties", {})
        required = set(params.get("required", []))

        if not props:
            lines.append(f"  type {name} = () => any;")
        else:
            param_lines = []
            for pname, pdef in props.items():
                ptype = pdef.get("type", "any")
                ts_type = {"string": "string", "number": "number", "boolean": "boolean",
                          "integer": "number", "array": "any[]", "object": "object"}.get(ptype, "any")
                optional = "?" if pname not in required else ""
                pdesc = pdef.get("description", "")
                comment = f" // {pdesc}" if pdesc else ""
                param_lines.append(f"    {pname}{optional}: {ts_type},{comment}")

            # Use string concatenation to avoid Jinja template confusion with braces
            lines.append("  type " + name + " = (_: {")
            lines.extend(param_lines)
            lines.append("  }) => any;")
        lines.append("")

    lines.append("} // namespace functions")
    return "\n".join(lines)


def inject_tools_into_messages(messages: list, tools: list) -> list:
    """
    Inject harmony namespace definition into messages when tools are provided.
    Tools go into the developer message per harmony spec.
    """
    if not tools:
        return messages

    namespace_def = convert_tools_to_harmony_namespace(tools)
    messages = list(messages)  # Copy to avoid mutating original

    # Find or create developer message
    has_developer = any(m.get('role') == 'developer' for m in messages)
    if has_developer:
        for i, m in enumerate(messages):
            if m.get('role') == 'developer':
                messages[i] = dict(m)
                messages[i]['content'] = namespace_def + "\n\n" + m.get('content', '')
                break
    else:
        messages.insert(0, {'role': 'developer', 'content': namespace_def})

    return messages


def parse_harmony_response(text: str) -> dict:
    """
    Parse harmony format response to extract channels and tool calls.
    Uses the official openai-harmony package.

    Returns dict with:
    - 'final': user-facing response
    - 'reasoning': chain-of-thought (analysis channel)
    - 'tool_calls': list of tool calls [{id, type, function: {name, arguments}}]
    - 'parse_error': error message if parsing failed (with raw text as fallback)

    Args:
        text: Raw text from TensorRT-LLM output (with skip_special_tokens=False)
    """
    # TensorRT-LLM returns only the completion, not the generation prompt.
    # The harmony parser expects <|start|>assistant at the beginning, but TensorRT-LLM
    # excludes it since it was part of the prompt. Prepend it for proper parsing.
    # See: https://github.com/openai/harmony/issues/80
    if not text.startswith("<|start|>"):
        text = "<|start|>assistant" + text

    try:
        tokens = _harmony_enc.encode(text, allowed_special="all")
        messages = _harmony_enc.parse_messages_from_completion_tokens(
            tokens, role=Role.ASSISTANT, strict=False
        )
    except Exception as e:
        # Raise with detailed error info - let the endpoint return a proper HTTP error
        logger.error(f"Harmony parsing failed: {e}")
        logger.error(f"Raw text: {text}")
        raise ValueError(f"Harmony parsing failed: {e}") from e

    final_parts = []
    reasoning_parts = []
    tool_calls = []
    tool_call_counter = 0

    for msg in messages:
        channel = getattr(msg, 'channel', None)
        recipient = getattr(msg, 'recipient', None)
        content = msg.content

        # Extract text content
        if isinstance(content, list):
            content = ''.join(
                c.text if hasattr(c, 'text') else str(c)
                for c in content
            )

        if channel == 'final':
            final_parts.append(content)
        elif channel == 'analysis':
            reasoning_parts.append(content)
        elif channel == 'commentary' and recipient and recipient.startswith('functions.'):
            # This is a tool call: commentary to=functions.xxx
            func_name = recipient.replace('functions.', '')
            tool_call_counter += 1
            tool_calls.append({
                "id": f"call_{tool_call_counter:04d}_{int(time.time() * 1000) % 10000}",
                "type": "function",
                "function": {
                    "name": func_name,
                    "arguments": content
                }
            })

    return {
        "final": ''.join(final_parts),
        "reasoning": ''.join(reasoning_parts) if reasoning_parts else None,
        "tool_calls": tool_calls if tool_calls else None
    }
{% endraw %}


async def _handle_batch_completions(body: dict, batch_requests: list):
    """
    Handle batch completions with true GPU batching.

    This function processes multiple requests in a single GPU batch operation,
    providing significant speedup over sequential processing.

    Request format (via /v1/chat/completions):
    {
        "batch": [
            {"messages": [...]},
            {"messages": [...]}
        ],
        "max_tokens": 512,      # Shared defaults (can be overridden per request)
        "temperature": 0.7,
        "top_p": 0.9
    }

    Response format:
    {
        "object": "batch.completion",
        "choices": [
            {"index": 0, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"},
            {"index": 1, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"}
        ],
        "usage": {...},
        "batch_info": {"count": N, "processing_time_ms": X}
    }
    """
    import gc
    import asyncio
    from concurrent.futures import ThreadPoolExecutor

    start_time = time.time()

    # Shared defaults from outer request
    default_temperature = body.get('temperature', 0.7)
    default_max_tokens = body.get('max_tokens', 512)
    default_top_p = body.get('top_p', 0.9)
    include_reasoning = body.get('include_reasoning', False)

    total_requests = len(batch_requests)
    logger.info(f"=== Batch Request via /v1/chat/completions === total={total_requests}")

    if total_requests == 0:
        return JSONResponse(
            status_code=400,
            content={"error": {"message": "Empty batch", "type": "invalid_request"}}
        )

    # Prepare all prompts
    all_prompts = []
    for req in batch_requests:
        messages = req.get('messages', [])
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        all_prompts.append(prompt)

    # Use first request's params (or defaults) for batch
    first_req = batch_requests[0]
    sampling_params = SamplingParams(
        temperature=first_req.get('temperature', default_temperature),
        max_tokens=first_req.get('max_tokens', default_max_tokens),
        top_p=first_req.get('top_p', default_top_p),
        skip_special_tokens=False,
        stop_token_ids=HARMONY_STOP_TOKEN_IDS
    )

    # Process in chunks to prevent OOM (same as /v1/batch/completions)
    all_choices = []
    chunks_processed = 0

    # Thread pool for running sync generate without blocking event loop
    executor = ThreadPoolExecutor(max_workers=1)

    def sync_generate_batch(prompts, params):
        """Run sync generate in thread to preserve true GPU batching"""
        return llm.generate(prompts, params)

    for i in range(0, total_requests, MAX_BATCH_SIZE):
        chunk_prompts = all_prompts[i:i + MAX_BATCH_SIZE]
        chunk_size = len(chunk_prompts)

        logger.info(f"Processing batch chunk {chunks_processed + 1}: {chunk_size} prompts")

        # Run true GPU batch in thread pool
        loop = asyncio.get_event_loop()
        outputs = await loop.run_in_executor(
            executor,
            sync_generate_batch,
            chunk_prompts,
            sampling_params
        )

        # Process responses
        for j, output in enumerate(outputs):
            raw_text = output.outputs[0].text
            finish_reason = getattr(output.outputs[0], 'finish_reason', 'stop')
            parsed = parse_harmony_response(raw_text)

            choice = {
                "index": i + j,
                "message": {
                    "role": "assistant",
                    "content": parsed["final"]
                },
                "finish_reason": str(finish_reason) if finish_reason else "stop"
            }

            # Optionally include reasoning
            if include_reasoning and parsed.get("reasoning"):
                choice["reasoning"] = parsed["reasoning"]

            all_choices.append(choice)

        chunks_processed += 1
        gc.collect()

    processing_time_ms = int((time.time() - start_time) * 1000)
    logger.info(f"Batch completed: {total_requests} prompts, {processing_time_ms}ms ({processing_time_ms/total_requests:.0f}ms/prompt)")

    # Return OpenAI-like response with batch info
    return JSONResponse({
        "id": f"batch-{int(time.time() * 1000)}",
        "object": "batch.completion",
        "created": int(time.time()),
        "model": MODEL_ID,
        "choices": all_choices,
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        },
        "batch_info": {
            "count": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests,
            "chunks_processed": chunks_processed
        }
    })


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAI-compatible chat completions endpoint using TensorRT-LLM

    Supports both single and batch requests:
    - Single: {"messages": [...], "max_tokens": 512, ...}
    - Batch:  {"batch": [{"messages": [...]}, {"messages": [...]}], "max_tokens": 512, ...}

    Also supports OpenAI-compatible tool calling:
    - tools: [{"type": "function", "function": {...}}]
    - Returns tool_calls in response when model wants to call a function

    Batch mode uses true GPU batching for maximum throughput.
    This allows batch requests to flow through LiteLLM unchanged.
    """
    try:
        body = await request.json()

        # Check for batch mode - array of requests in "batch" field
        batch_requests = body.get('batch')
        if batch_requests and isinstance(batch_requests, list):
            return await _handle_batch_completions(body, batch_requests)

        # Single request mode (standard OpenAI format)
        messages = body.get('messages', [])
        tools = body.get('tools', None)  # OpenAI-compatible tools parameter
        temperature = body.get('temperature', 0.7)
        max_tokens = body.get('max_tokens', 512)
        stream = body.get('stream', False)
        top_p = body.get('top_p', 0.9)
        include_reasoning = body.get('include_reasoning', False)  # Optional: include chain-of-thought

        # If tools provided, inject harmony namespace into messages
        if tools:
            messages = inject_tools_into_messages(messages, tools)
            logger.info(f"=== Tools Injected === {len(tools)} tools converted to harmony namespace")

        # Apply chat template (uses harmony format for gpt-oss, chatml for others, etc.)
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        logger.info(f"=== API Request ===")
        logger.info(f"max_tokens: {max_tokens}, temperature: {temperature}, top_p: {top_p}")
        logger.info(f"Prompt length (chars): {len(prompt)}")
        if tools:
            logger.info(f"Tools: {[t.get('function', {}).get('name') for t in tools if t.get('type') == 'function']}")

        # Configure sampling parameters
        # skip_special_tokens=False preserves harmony format tokens for parsing
        # stop_token_ids ensures model stops at <|return|> or <|call|> tokens
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            skip_special_tokens=False,
            stop_token_ids=HARMONY_STOP_TOKEN_IDS
        )

        # Generate with TensorRT-LLM (async to handle concurrent requests)
        output = await llm.generate_async(prompt, sampling_params)
        raw_text = output.outputs[0].text
        finish_reason = getattr(output.outputs[0], 'finish_reason', 'unknown')

        logger.info(f"=== Raw Response ===")
        logger.info(f"Length (chars): {len(raw_text)}")
        logger.info(f"Finish reason: {finish_reason}")
        logger.info(f"RAW_RESPONSE_START>>>{raw_text}<<<RAW_RESPONSE_END")

        # Parse harmony format to extract final response and tool calls
        parsed = parse_harmony_response(raw_text)
        response_text = parsed["final"]
        tool_calls = parsed.get("tool_calls")

        logger.info(f"=== Parsed Response ===")
        logger.info(f"Harmony parsing applied: {parsed['reasoning'] is not None}")
        logger.info(f"Final length (chars): {len(response_text)}")
        logger.info(f"Tool calls detected: {len(tool_calls) if tool_calls else 0}")
        if tool_calls:
            logger.info(f"Tool calls: {[tc['function']['name'] for tc in tool_calls]}")
        logger.info(f"PARSED_RESPONSE_START>>>{response_text}<<<PARSED_RESPONSE_END")

        # Optionally include reasoning in a separate field
        reasoning_text = parsed["reasoning"] if include_reasoning else None

        # Generate unique ID
        completion_id = f"chatcmpl-{int(time.time() * 1000)}"
        created = int(time.time())

        if stream:
            # Streaming response (simplified - sends full response in one chunk)
            async def generate_stream():
                # Build message content
                message_content = {"role": "assistant"}
                if tool_calls:
                    message_content["tool_calls"] = tool_calls
                    if response_text:
                        message_content["content"] = response_text
                else:
                    message_content["content"] = response_text

                chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": message_content,
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"

                # Final chunk with finish_reason
                final_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "tool_calls" if tool_calls else "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )

        # Non-streaming response
        # Build message based on whether tool calls were made
        message = {"role": "assistant"}
        if tool_calls:
            message["tool_calls"] = tool_calls
            # Content can be null or contain text when there are tool calls
            message["content"] = response_text if response_text else None
            api_finish_reason = "tool_calls"
        else:
            message["content"] = response_text
            api_finish_reason = "stop"

        response_data = {
            "id": completion_id,
            "object": "chat.completion",
            "created": created,
            "model": MODEL_ID,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": api_finish_reason
            }],
            "usage": {
                "prompt_tokens": 0,  # TensorRT-LLM doesn't expose token counts easily
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }

        # Optionally include reasoning (chain-of-thought) in response
        if reasoning_text:
            response_data["reasoning"] = reasoning_text

        return JSONResponse(response_data)

    except Exception as e:
        logger.error(f"Error in chat completions: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


@app.get("/v1/models")
async def openai_models():
    """OpenAI-compatible models endpoint - returns available LLM model"""
    return JSONResponse({
        "object": "list",
        "data": [{
            "id": MODEL_ID,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "thinkube",
            "permission": [],
            "root": MODEL_ID,
            "parent": None
        }]
    })


# ============================================================================
# Batch Completions Endpoint (for parallel data generation)
# ============================================================================

# DGX Spark unified memory (128GB): 20B NVFP4 model uses ~15GB, leaving ~100GB for KV cache
# With 80Gi pod memory limit, batch size 8 is safe for 800 max_tokens
MAX_BATCH_SIZE = 8

@app.post("/v1/batch/completions")
async def batch_completions(request: Request):
    """
    Batch completions endpoint for parallel inference.

    Accepts multiple prompts and processes them in GPU batches,
    providing significant speedup over sequential requests.

    Large batches are automatically chunked to prevent OOM on unified memory.

    Request format:
    {
        "requests": [
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            ...
        ]
    }

    Response format:
    {
        "responses": [
            {"content": "...", "finish_reason": "stop"},
            {"content": "...", "finish_reason": "stop"},
            ...
        ],
        "batch_size": N,
        "processing_time_ms": X,
        "chunks_processed": Y
    }
    """
    import gc

    try:
        start_time = time.time()
        body = await request.json()
        requests_list = body.get('requests', [])

        if not requests_list:
            return JSONResponse(
                status_code=400,
                content={"error": {"message": "No requests provided", "type": "invalid_request"}}
            )

        total_requests = len(requests_list)
        logger.info(f"=== Batch Request === total={total_requests}, max_chunk={MAX_BATCH_SIZE}")

        # Prepare all prompts and sampling params
        all_prompts = []
        all_params = []

        for req in requests_list:
            messages = req.get('messages', [])
            temperature = req.get('temperature', 0.7)
            max_tokens = req.get('max_tokens', 512)
            top_p = req.get('top_p', 0.9)

            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            all_prompts.append(prompt)
            all_params.append(SamplingParams(
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                skip_special_tokens=False,
                stop_token_ids=HARMONY_STOP_TOKEN_IDS
            ))

        # Process in chunks to prevent OOM
        all_responses = []
        chunks_processed = 0

        import asyncio
        from concurrent.futures import ThreadPoolExecutor

        # Use thread pool to run sync generate() without blocking event loop
        # This preserves true GPU batching while being async-compatible
        executor = ThreadPoolExecutor(max_workers=1)

        def sync_generate_batch(prompts, params):
            """Run sync generate in thread to avoid blocking uvicorn event loop"""
            return llm.generate(prompts, params)

        for i in range(0, total_requests, MAX_BATCH_SIZE):
            chunk_prompts = all_prompts[i:i + MAX_BATCH_SIZE]
            chunk_params = all_params[i:i + MAX_BATCH_SIZE]
            chunk_size = len(chunk_prompts)

            logger.info(f"Processing chunk {chunks_processed + 1}: {chunk_size} prompts (indices {i}-{i + chunk_size - 1})")

            # Use first request's params for this chunk (TensorRT-LLM batches internally)
            batch_sampling_params = chunk_params[0]

            # Run sync generate in thread pool to preserve true GPU batching
            # while not blocking the uvicorn event loop
            loop = asyncio.get_event_loop()
            outputs = await loop.run_in_executor(
                executor,
                sync_generate_batch,
                chunk_prompts,
                batch_sampling_params
            )

            # Process responses
            for output in outputs:
                raw_text = output.outputs[0].text
                finish_reason = getattr(output.outputs[0], 'finish_reason', 'stop')
                parsed = parse_harmony_response(raw_text)

                all_responses.append({
                    "content": parsed["final"],
                    "finish_reason": str(finish_reason) if finish_reason else "stop",
                    "reasoning": parsed.get("reasoning")
                })

            chunks_processed += 1
            gc.collect()

        processing_time_ms = int((time.time() - start_time) * 1000)
        logger.info(f"Batch completed: {total_requests} prompts in {chunks_processed} chunks, {processing_time_ms}ms total ({processing_time_ms/total_requests:.0f}ms/prompt)")

        return JSONResponse({
            "responses": all_responses,
            "batch_size": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests,
            "chunks_processed": chunks_processed,
            "max_chunk_size": MAX_BATCH_SIZE
        })

    except Exception as e:
        logger.error(f"Batch error: {e}")
        gc.collect()
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


# Mount Gradio app with Thinkube favicon
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Initialize model (must be in main block for MPI support)
    initialize_model()

    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
