#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with MXFP4 support on Blackwell (DGX Spark GB10)
Uses trtllm-serve backend for optimized MXFP4 inference
"""

import os
import json
import time
import logging
import httpx
import gradio as gr
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from transformers import AutoTokenizer
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Note: trtllm-serve handles harmony format parsing internally
# It returns structured content, reasoning, and tool_calls fields

# Harmony format stop tokens (from openai-harmony spec)
# These tokens indicate model should stop generating:
# - <|return|> (200002): Model is done with final response
# - <|call|> (200012): Model wants to call a tool
HARMONY_STOP_TOKENS = ["<|return|>", "<|call|>"]

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Model configuration from template parameter
MODEL_ID = "{{ model_id }}"
MODEL_PATH = os.environ.get('MODEL_PATH')

# trtllm-serve backend URL (started by entrypoint.sh)
TRTLLM_BACKEND_URL = "http://127.0.0.1:8355"

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# Global tokenizer (for chat template formatting)
tokenizer = None

# HTTP client for backend calls (with connection pooling)
http_client = None

def initialize():
    """Initialize tokenizer and HTTP client"""
    global tokenizer, http_client
    print(f"Model ID: {MODEL_ID}")
    print(f"Model path: {MODEL_PATH}")
    print(f"Backend URL: {TRTLLM_BACKEND_URL}")

    # Load tokenizer for chat template formatting
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    # Initialize HTTP client with connection pooling for backend calls
    http_client = httpx.AsyncClient(
        base_url=TRTLLM_BACKEND_URL,
        timeout=httpx.Timeout(300.0, connect=10.0),  # 5 min for generation, 10s for connect
    )

    print(f"âœ“ Tokenizer loaded, connected to trtllm-serve backend")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response via trtllm-serve backend with harmony chat template"""
    import httpx

    # Gradio 6.x uses OpenAI-style message format for history
    # history is already a list of {"role": "user"|"assistant", "content": ...}
    messages = list(history)  # Copy history

    # Add current message
    messages.append({"role": "user", "content": message})

    # Call trtllm-serve backend (OpenAI-compatible API)
    # The backend handles chat template application internally
    with httpx.Client(base_url=TRTLLM_BACKEND_URL, timeout=300.0) as client:
        response = client.post(
            "/v1/chat/completions",
            json={
                "model": MODEL_ID,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": 0.9,
                "stop": HARMONY_STOP_TOKENS,
            }
        )
        response.raise_for_status()
        result = response.json()

    # Extract response from trtllm-serve (already parsed harmony format)
    msg = result["choices"][0]["message"]
    content = msg.get("content") or ""
    yield content

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if trtllm-serve backend is healthy
        response = await http_client.get("/health")
        backend_health = response.json() if response.status_code == 200 else {"status": "unknown"}
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "model_path": MODEL_PATH,
            "engine": "trtllm-serve (MXFP4)",
            "backend": backend_health
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


# ============================================================================
# OpenAI-Compatible API Endpoints (for LiteLLM integration)
# ============================================================================
# Note: trtllm-serve handles harmony format parsing internally and returns:
# - message.content: final user-facing response
# - message.reasoning: chain-of-thought (analysis channel)
# - message.tool_calls: list of tool calls [{id, type, function: {name, arguments}}]


async def _handle_batch_completions(body: dict, batch_requests: list):
    """
    Handle batch completions via trtllm-serve backend.

    Processes requests sequentially through the backend (backend handles batching internally).

    Request format (via /v1/chat/completions):
    {
        "batch": [
            {"messages": [...]},
            {"messages": [...]}
        ],
        "max_tokens": 512,      # Shared defaults (can be overridden per request)
        "temperature": 0.7,
        "top_p": 0.9
    }

    Response format:
    {
        "object": "batch.completion",
        "choices": [
            {"index": 0, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"},
            {"index": 1, "message": {"role": "assistant", "content": "..."}, "finish_reason": "stop"}
        ],
        "usage": {...},
        "batch_info": {"count": N, "processing_time_ms": X}
    }
    """
    import asyncio

    start_time = time.time()

    # Shared defaults from outer request
    default_temperature = body.get('temperature', 0.7)
    default_max_tokens = body.get('max_tokens', 512)
    default_top_p = body.get('top_p', 0.9)
    include_reasoning = body.get('include_reasoning', False)

    total_requests = len(batch_requests)
    logger.info(f"=== Batch Request via /v1/chat/completions === total={total_requests}")

    if total_requests == 0:
        return JSONResponse(
            status_code=400,
            content={"error": {"message": "Empty batch", "type": "invalid_request"}}
        )

    # Process requests concurrently via backend
    async def process_single_request(idx: int, req: dict):
        messages = req.get('messages', [])
        temperature = req.get('temperature', default_temperature)
        max_tokens = req.get('max_tokens', default_max_tokens)
        top_p = req.get('top_p', default_top_p)

        response = await http_client.post(
            "/v1/chat/completions",
            json={
                "model": MODEL_ID,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": top_p,
                "stop": HARMONY_STOP_TOKENS,
            }
        )
        response.raise_for_status()
        result = response.json()

        # trtllm-serve returns structured harmony response
        msg = result["choices"][0]["message"]
        content = msg.get("content") or ""
        reasoning = msg.get("reasoning")
        finish_reason = result["choices"][0].get("finish_reason", "stop")

        choice = {
            "index": idx,
            "message": {
                "role": "assistant",
                "content": content
            },
            "finish_reason": finish_reason
        }

        if include_reasoning and reasoning:
            choice["reasoning"] = reasoning

        return choice

    # Run all requests concurrently (backend handles batching)
    tasks = [process_single_request(i, req) for i, req in enumerate(batch_requests)]
    all_choices = await asyncio.gather(*tasks)

    # Sort by index to maintain order
    all_choices = sorted(all_choices, key=lambda x: x["index"])

    processing_time_ms = int((time.time() - start_time) * 1000)
    logger.info(f"Batch completed: {total_requests} prompts, {processing_time_ms}ms ({processing_time_ms/total_requests:.0f}ms/prompt)")

    # Return OpenAI-like response with batch info
    return JSONResponse({
        "id": f"batch-{int(time.time() * 1000)}",
        "object": "batch.completion",
        "created": int(time.time()),
        "model": MODEL_ID,
        "choices": all_choices,
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        },
        "batch_info": {
            "count": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests
        }
    })


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAI-compatible chat completions endpoint via trtllm-serve

    Supports both single and batch requests:
    - Single: {"messages": [...], "max_tokens": 512, ...}
    - Batch:  {"batch": [{"messages": [...]}, {"messages": [...]}], "max_tokens": 512, ...}

    Also supports OpenAI-compatible tool calling:
    - tools: [{"type": "function", "function": {...}}]
    - Returns tool_calls in response when model wants to call a function

    Proxies to trtllm-serve backend and applies harmony format parsing.
    """
    try:
        body = await request.json()

        # Check for batch mode - array of requests in "batch" field
        batch_requests = body.get('batch')
        if batch_requests and isinstance(batch_requests, list):
            return await _handle_batch_completions(body, batch_requests)

        # Single request mode (standard OpenAI format)
        messages = body.get('messages', [])
        tools = body.get('tools', None)  # OpenAI-compatible tools parameter
        temperature = body.get('temperature', 0.7)
        max_tokens = body.get('max_tokens', 512)
        stream = body.get('stream', False)
        top_p = body.get('top_p', 0.9)
        include_reasoning = body.get('include_reasoning', False)  # Optional: include chain-of-thought

        logger.info(f"=== API Request ===")
        logger.info(f"max_tokens: {max_tokens}, temperature: {temperature}, top_p: {top_p}")
        logger.info(f"Messages: {len(messages)}")
        if tools:
            logger.info(f"Tools: {[t.get('function', {}).get('name') for t in tools if t.get('type') == 'function']}")

        # Build request payload for backend
        # trtllm-serve handles chat template application
        backend_payload = {
            "model": MODEL_ID,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stop": HARMONY_STOP_TOKENS,
        }
        if tools:
            backend_payload["tools"] = tools

        # Call trtllm-serve backend
        response = await http_client.post("/v1/chat/completions", json=backend_payload)
        response.raise_for_status()
        result = response.json()

        # trtllm-serve returns structured harmony response
        msg = result["choices"][0]["message"]
        response_text = msg.get("content") or ""
        reasoning_text = msg.get("reasoning") if include_reasoning else None
        tool_calls = msg.get("tool_calls") or None
        # Convert empty list to None for consistency
        if tool_calls is not None and len(tool_calls) == 0:
            tool_calls = None
        finish_reason = result["choices"][0].get("finish_reason", "stop")

        logger.info(f"=== Response from trtllm-serve ===")
        logger.info(f"Content length (chars): {len(response_text)}")
        logger.info(f"Reasoning: {reasoning_text is not None}")
        logger.info(f"Tool calls: {len(tool_calls) if tool_calls else 0}")
        logger.info(f"Finish reason: {finish_reason}")
        if tool_calls:
            logger.info(f"Tool call names: {[tc['function']['name'] for tc in tool_calls]}")

        # Generate unique ID
        completion_id = f"chatcmpl-{int(time.time() * 1000)}"
        created = int(time.time())

        if stream:
            # Streaming response (simplified - sends full response in one chunk)
            async def generate_stream():
                # Build message content
                message_content = {"role": "assistant"}
                if tool_calls:
                    message_content["tool_calls"] = tool_calls
                    if response_text:
                        message_content["content"] = response_text
                else:
                    message_content["content"] = response_text

                chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": message_content,
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"

                # Final chunk with finish_reason
                final_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": MODEL_ID,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "tool_calls" if tool_calls else "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )

        # Non-streaming response
        # Build message based on whether tool calls were made
        message = {"role": "assistant"}
        if tool_calls:
            message["tool_calls"] = tool_calls
            # Content can be null or contain text when there are tool calls
            message["content"] = response_text if response_text else None
            api_finish_reason = "tool_calls"
        else:
            message["content"] = response_text
            api_finish_reason = "stop"

        response_data = {
            "id": completion_id,
            "object": "chat.completion",
            "created": created,
            "model": MODEL_ID,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": api_finish_reason
            }],
            "usage": result.get("usage", {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            })
        }

        # Optionally include reasoning (chain-of-thought) in response
        if reasoning_text:
            response_data["reasoning"] = reasoning_text

        return JSONResponse(response_data)

    except Exception as e:
        logger.error(f"Error in chat completions: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


@app.get("/v1/models")
async def openai_models():
    """OpenAI-compatible models endpoint - returns available LLM model"""
    return JSONResponse({
        "object": "list",
        "data": [{
            "id": MODEL_ID,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "thinkube",
            "permission": [],
            "root": MODEL_ID,
            "parent": None
        }]
    })


# ============================================================================
# Batch Completions Endpoint (for parallel data generation)
# ============================================================================

@app.post("/v1/batch/completions")
async def batch_completions(request: Request):
    """
    Batch completions endpoint for parallel inference via trtllm-serve.

    Accepts multiple prompts and processes them concurrently.
    Backend handles GPU batching internally.

    Request format:
    {
        "requests": [
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            {"messages": [...], "max_tokens": 512, "temperature": 0.7},
            ...
        ]
    }

    Response format:
    {
        "responses": [
            {"content": "...", "finish_reason": "stop"},
            {"content": "...", "finish_reason": "stop"},
            ...
        ],
        "batch_size": N,
        "processing_time_ms": X
    }
    """
    import asyncio

    try:
        start_time = time.time()
        body = await request.json()
        requests_list = body.get('requests', [])

        if not requests_list:
            return JSONResponse(
                status_code=400,
                content={"error": {"message": "No requests provided", "type": "invalid_request"}}
            )

        total_requests = len(requests_list)
        logger.info(f"=== Batch Request === total={total_requests}")

        # Process requests concurrently via backend
        async def process_single_request(idx: int, req: dict):
            messages = req.get('messages', [])
            temperature = req.get('temperature', 0.7)
            max_tokens = req.get('max_tokens', 512)
            top_p = req.get('top_p', 0.9)

            response = await http_client.post(
                "/v1/chat/completions",
                json={
                    "model": MODEL_ID,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "top_p": top_p,
                    "stop": HARMONY_STOP_TOKENS,
                }
            )
            response.raise_for_status()
            result = response.json()

            # trtllm-serve returns structured harmony response
            msg = result["choices"][0]["message"]
            content = msg.get("content") or ""
            reasoning = msg.get("reasoning")
            finish_reason = result["choices"][0].get("finish_reason", "stop")

            return {
                "index": idx,
                "content": content,
                "finish_reason": finish_reason,
                "reasoning": reasoning
            }

        # Run all requests concurrently
        tasks = [process_single_request(i, req) for i, req in enumerate(requests_list)]
        results = await asyncio.gather(*tasks)

        # Sort by index and remove index field
        results = sorted(results, key=lambda x: x["index"])
        all_responses = [{k: v for k, v in r.items() if k != "index"} for r in results]

        processing_time_ms = int((time.time() - start_time) * 1000)
        logger.info(f"Batch completed: {total_requests} prompts, {processing_time_ms}ms total ({processing_time_ms/total_requests:.0f}ms/prompt)")

        return JSONResponse({
            "responses": all_responses,
            "batch_size": total_requests,
            "processing_time_ms": processing_time_ms,
            "avg_time_per_prompt_ms": processing_time_ms / total_requests
        })

    except Exception as e:
        logger.error(f"Batch error: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_error"}}
        )


# Mount Gradio app with Thinkube theme and favicon (Gradio 6.x: theme/css go in mount_gradio_app)
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Initialize tokenizer and HTTP client
    initialize()

    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
