#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with NVFP4 support on Blackwell (DGX Spark GB10)
"""

import os
import gradio as gr
from fastapi import FastAPI
import uvicorn
from openai import OpenAI
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Model configuration from template
MODEL_ID = "{{ model_id }}"

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# OpenAI client pointing to local TensorRT-LLM server
# trtllm-serve provides OpenAI-compatible API on port 8000
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy-key"  # TensorRT-LLM doesn't require real API key
)

print(f"Connecting to TensorRT-LLM server for model: {MODEL_ID}")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response using TensorRT-LLM via OpenAI-compatible API"""
    # Build messages from history
    messages = []
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})
    messages.append({"role": "user", "content": message})

    # Generate with TensorRT-LLM
    stream = client.chat.completions.create(
        model=MODEL_ID,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True
    )

    # Stream response for Gradio
    response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            response += chunk.choices[0].delta.content
            yield response

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if TensorRT-LLM server is responding
        models = client.models.list()
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "engine": "TensorRT-LLM"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

# Mount Gradio app with Thinkube favicon
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
