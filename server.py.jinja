#!/usr/bin/env python3
"""
TensorRT-LLM Inference Server with Gradio UI
For text generation models with NVFP4 support on Blackwell (DGX Spark GB10)
Uses TensorRT-LLM Python API for direct model loading
"""

import os
import gradio as gr
from fastapi import FastAPI
import uvicorn
from tensorrt_llm import LLM, SamplingParams
from thinkube_theme import create_thinkube_theme, THINKUBE_CSS

# Model configuration from template parameter
MODEL_ID = "{{ model_id }}"
MODEL_PATH = os.environ.get('MODEL_PATH')

# Initialize FastAPI app
app = FastAPI(title="{{ project_name }} TensorRT-LLM Server")

# Global LLM instance (initialized in main block to support MPI)
llm = None

def initialize_model():
    """Initialize TensorRT-LLM model (must be called from main block for MPI support)"""
    global llm
    print(f"Loading model from: {MODEL_PATH}")
    print(f"Model ID: {MODEL_ID}")
    llm = LLM(model=MODEL_PATH)
    print(f"âœ“ TensorRT-LLM model loaded successfully")

def generate_response(message: str, history: list, temperature: float = 0.7, max_tokens: int = 512):
    """Generate response using TensorRT-LLM Python API"""
    # Build conversation context from history
    conversation = []
    for human, assistant in history:
        conversation.append(f"User: {human}\nAssistant: {assistant}")

    # Add current message
    conversation.append(f"User: {message}\nAssistant:")
    prompt = "\n\n".join(conversation)

    # Configure sampling parameters
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=0.9
    )

    # Generate with TensorRT-LLM Python API
    outputs = llm.generate([prompt], sampling_params)

    # Extract response
    response = outputs[0].outputs[0].text
    yield response

# Create Thinkube-styled Gradio interface
thinkube_theme = create_thinkube_theme()

demo = gr.ChatInterface(
    generate_response,
    title="{{ project_title | default(project_name) }}",
    description=f"Chat with {MODEL_ID} (powered by TensorRT-LLM with NVFP4)",
    examples=[
        ["Hello! How are you?", 0.7, 512],
        ["Can you explain quantum computing in simple terms?", 0.7, 512],
        ["Write a Python function to calculate fibonacci numbers", 0.7, 512],
    ],
    theme=thinkube_theme,
    css=THINKUBE_CSS,
    analytics_enabled=False,
    additional_inputs=[
        gr.Slider(0.1, 2.0, value=0.7, label="Temperature"),
        gr.Slider(64, 2048, value=512, label="Max Tokens"),
    ],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    try:
        # Check if model is loaded
        return {
            "status": "healthy",
            "model": MODEL_ID,
            "model_path": MODEL_PATH,
            "engine": "TensorRT-LLM (Python API)"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

# Mount Gradio app with Thinkube favicon
app = gr.mount_gradio_app(
    app,
    demo,
    path="/",
    favicon_path="/app/icons/tk_ai.png"  # Thinkube AI icon
)

if __name__ == "__main__":
    # Initialize model (must be in main block for MPI support)
    initialize_model()

    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        log_level="info",
    )
